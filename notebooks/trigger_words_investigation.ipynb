{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this notebook is to identify suspect words that appear on the wikipedia pages of 'bad actors' i.e. slave traders.\n",
    "We will use the examples found from cross referencing with the Wikipedia list of slave traders and the Wikidata entries with the property of the 'Legacies of a British Slave Trader'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = '../data/open-plaques-subjects-london.csv'\n",
    "df = pd.read_csv(FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focus on the records that have a Wikipedia URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset = ['en_wikipedia_url'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_contents(url):\n",
    "    name = url.split('/')[-1]\n",
    "    try:\n",
    "        contents = wikipedia.page(name).content\n",
    "        return contents \n",
    "    except:\n",
    "        print('Wiki retrieval failed {0}'.format(name))\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Know bad slave traders from wikipedia list cross reference and wikidata attribute\n",
    "know_names = ['James Brown', 'William Penn', 'John Marshall', 'Robert Milligan',\n",
    "              'Martin Van Buren', 'Benjamin Franklin', \n",
    "              'Quintin Hogg', 'William Ewart Gladstone', 'Elizabeth Barrett Browning']\n",
    "\n",
    "content = ''\n",
    "\n",
    "for name in know_names:\n",
    "    url = df.loc[df['full_name'] == name, 'en_wikipedia_url'].iloc[0]\n",
    "    n_content = get_wiki_contents(url)\n",
    "    content = content + n_content.lower()\n",
    "    print(len(n_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) \n",
    "word_tokens = word_tokenize(content) \n",
    "  \n",
    "filtered_sentences = [w for w in word_tokens if not w in stop_words] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out the punctuation\n",
    "nonPunct = re.compile('.*[A-Za-z].*')  # must contain a letter\n",
    "filtered = [w for w in filtered_sentences if nonPunct.match(w)]\n",
    "counts = Counter(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sorted(counts.items(), key=lambda item: item[1])[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Know bad slave traders from wikipedia list cross reference and wikidata attribute\n",
    "know_names = ['James Brown','William Penn','John Marshall','Robert Milligan','Martin Van Buren','Benjamin Franklin', \n",
    "              'Quintin Hogg', 'William Ewart Gladstone']#, 'Elizabeth Barrett Browning']\n",
    "\n",
    "all_freqs = []\n",
    "for name in know_names:\n",
    "    url = df.loc[df['full_name'] == name, 'en_wikipedia_url'].iloc[0]\n",
    "    content = get_wiki_contents(url).lower()\n",
    "    \n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    word_tokens = word_tokenize(content) \n",
    "\n",
    "    filtered_sentences = [w for w in word_tokens if not w in stop_words] \n",
    "\n",
    "    # Filter out the punctuation\n",
    "    nonPunct = re.compile('.*[A-Za-z].*')  # must contain a letter\n",
    "    filtered = [w for w in filtered_sentences if nonPunct.match(w)]\n",
    "    counts = Counter(filtered)\n",
    "    \n",
    "    freqs = [pair[0] for pair in sorted(counts.items(), key=lambda item: item[1])][::-1]\n",
    "    all_freqs.append(freqs)\n",
    "    print(freqs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = set([item for sublist in all_freqs for item in sublist])\n",
    "\n",
    "inter_count = {}\n",
    "for word in all_words:\n",
    "    inter_count[word] = 0\n",
    "    for freq in all_freqs:\n",
    "        if word in freq:\n",
    "            inter_count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(sorted(inter_count.items(), key=lambda item: item[1])[::-1][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
